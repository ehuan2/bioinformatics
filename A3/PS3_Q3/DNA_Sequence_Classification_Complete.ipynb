{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c299ea74",
   "metadata": {},
   "source": [
    "\n",
    "# Deep Learning on DNA Sequences\n",
    "\n",
    "You will build, train, and evaluate neural networks for taxonomic classification of DNA sequences. Specifically, you will load and process DNA sequences from a FASTA file, exploring sequence data, computing k-mers, and building a neural network for classification. \n",
    "\n",
    "---\n",
    "The notebook is prepared for Pytorch, but if you choose to use TensorFlow you may modify the notebook as you see fit   \n",
    "**You must complete the sections marked with an asterisc (*) with your own code** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d0681b",
   "metadata": {},
   "source": [
    "\n",
    "### 1.1 Reading and Exploring the FASTA Dataset\n",
    "\n",
    "We'll start by reading a FASTA file and extracting the DNA sequences and taxonomic labels from each entry.\n",
    "\n",
    "The headers in the file are of the form `>sample_i|taxonomic_class`. We'll parse the file, extract the classes, and explore the data distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cba63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# Read and parse the FASTA file\n",
    "fasta_file = \"HIV_data/train_data.fasta\"  # Replace with your file path\n",
    "sequences = []\n",
    "labels = []\n",
    "\n",
    "for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "    sequences.append(str(record.seq))\n",
    "    label = record.description.split('|')[1]  # Extract the taxonomic class\n",
    "    labels.append(label)\n",
    "\n",
    "# Plot the distribution of labels\n",
    "label_counts = Counter(labels)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(label_counts.keys(), label_counts.values())\n",
    "plt.xlabel('Taxonomic Class')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Taxonomic Classes')\n",
    "plt.show()\n",
    "\n",
    "# Plot the distribution of sequence lengths\n",
    "sequence_lengths = [len(seq) for seq in sequences]\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(sequence_lengths, bins=20, edgecolor='black')\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Sequence Lengths')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fb5014",
   "metadata": {},
   "source": [
    "\n",
    "### 1.2* Preprocessing DNA Sequences: Computing K-mer Frequencies\n",
    "\n",
    "K-mer frequencies can be used as input features for classification models. Let's compute the k-mer frequencies for each DNA sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca018639",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from itertools import product\n",
    "\n",
    "# Function to calculate k-mer frequency\n",
    "def kmer_frequency(sequence, k=3):\n",
    "    # Write your own implementation\n",
    "    pass \n",
    "    return   # Normalize frequencies\n",
    "\n",
    "# Compute k-mer frequencies for each sequence\n",
    "k = None  # Set k an appropriate value for k\n",
    "\n",
    "features = [kmer_frequency(seq, k) for seq in sequences]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060757cc",
   "metadata": {},
   "source": [
    "\n",
    "### 1.3* Defining and Training the Neural Network\n",
    "\n",
    "With the k-mer frequency features, we'll design a fully connected neural network for taxonomic classification using the architecture provided.\n",
    "\n",
    "#### Model Architecture: `Net_linear`\n",
    "I suggest a model with three fully connected layers with ReLU activations and Dropout for regularization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a4c2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "# Define the Net_linear model\n",
    "class Net_linear(nn.Module):\n",
    "    def __init__(self, n_input, n_output):\n",
    "        super(Net_linear, self).__init__()\n",
    "        # Define your network\n",
    "        pass \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Ensure input is flattened if necessary\n",
    "        pass\n",
    "\n",
    "# Define input and output dimensions\n",
    "n_input = len(features[0])  # Based on k-mer feature size\n",
    "n_output = len(set(labels))  # Number of unique classes\n",
    "\n",
    "# Initialize the model\n",
    "model = Net_linear(n_input=n_input, n_output=n_output)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda9c3a8",
   "metadata": {},
   "source": [
    "#### 1.4* Supervised Training of the model\n",
    "You may run the training process for as many epochs as you wish, but I suggest training for 100 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b723ae07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Set up the criterion and optimizer\n",
    "criterion = None\n",
    "optimizer = None\n",
    "\n",
    "# Convert features and labels to tensors and create DataLoader\n",
    "# Mapping labels to integers\n",
    "label_map = {label: idx for idx, label in enumerate(set(labels))}\n",
    "mapped_labels = [label_map[lbl] for lbl in labels]\n",
    "\n",
    "features_tensor = torch.tensor(features, dtype=torch.float32)\n",
    "labels_tensor = torch.tensor(mapped_labels, dtype=torch.long)\n",
    "\n",
    "dataset = TensorDataset(features_tensor, labels_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Define the Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in dataloader:\n",
    "        # Forward pass\n",
    "        loss = torch.tensor([0.0])  #placeholder for the loss value\n",
    "\n",
    "        # Backward pass and optimization\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Note: This model can be tuned further by adjusting batch size, learning rate, or Dropout rates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91566c3e",
   "metadata": {},
   "source": [
    "\n",
    "### 1.5 Evaluating on the Validation Set\n",
    "\n",
    "After training, it's important to evaluate the model on a separate test set to understand its performance on unseen data.\n",
    "\n",
    "We'll split our dataset further or use a held-out test set to compute accuracy, plot a confusion matrix, and evaluate other metrics if necessary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c69c783",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "fasta_file = \"HIV_data/val_data.fasta\"  # Replace with your file path\n",
    "sequences = []\n",
    "ids = []\n",
    "\n",
    "for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "    sequences.append(str(record.seq))\n",
    "    label = record.description[0]  # Extract the tid\n",
    "    ids.append(label)\n",
    "\n",
    "val_features = [kmer_frequency(seq, k) for seq in sequences]\n",
    "val_labels = pd.read_csv(\"HIV_data/val_taxonomy.csv\")[\"taxonomy\"].to_list()\n",
    "mapped_val_labels = [label_map[lbl] for lbl in val_labels]\n",
    "\n",
    "val_features = torch.tensor(val_features, dtype=torch.float32)\n",
    "val_labels_tensor = torch.tensor(mapped_val_labels, dtype=torch.long)\n",
    "\n",
    "# Create DataLoader for test set\n",
    "test_dataset = TensorDataset(val_features, val_labels_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Evaluation\n",
    "model.eval()  # Set model to evaluation mode\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.numpy())\n",
    "        all_labels.extend(targets.numpy())\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2a83bc",
   "metadata": {},
   "source": [
    "\n",
    "### 1.5 (Optional) Hyperparameter Tuning\n",
    "\n",
    "Tuning hyperparameters such as the learning rate, number of layers, or dropout rates can significantly impact model performance.\n",
    "\n",
    "Run a grid search to experiment with different learning rates and dropout probabilities. After each run, we can compare results to find the best configuration. That will make your model more robust and potentially will increase performance in the training set. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa65233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning - experimenting with different learning rates and dropout rates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6e0e73",
   "metadata": {},
   "source": [
    "\n",
    "## Part 2: Training a 1D Convolutional Neural Network (CNN)\n",
    "\n",
    "In this final part, we'll train a 1D CNN to classify DNA sequences. This approach involves:\n",
    "\n",
    "1. **Setting a Maximum Length**: We'll define a maximum sequence length, then trim or pad all sequences to this length.\n",
    "2. **One-Hot Encoding DNA Sequences**: Convert sequences to one-hot encoding, so that each base (A, C, G, T) is represented in a way compatible with convolutional layers.\n",
    "3. **Defining and Training the CNN**: We'll use the CNN architecture provided, with five convolutional layers and four fully connected layers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a24ca9",
   "metadata": {},
   "source": [
    "\n",
    "### 2.1 Setting a Maximum Sequence Length\n",
    "\n",
    "Convolutional networks require fixed-length inputs. We'll set a maximum sequence length and either trim longer sequences or pad shorter sequences to this length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae37e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Read and parse the FASTA file\n",
    "fasta_file = \"HIV_data/train_data.fasta\"  # Replace with your file path\n",
    "sequences = []\n",
    "labels = []\n",
    "\n",
    "for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "    sequences.append(str(record.seq))\n",
    "    label = record.description.split('|')[1]  # Extract the taxonomic class\n",
    "    labels.append(label)\n",
    "\n",
    "\n",
    "# Define the maximum length for the sequences\n",
    "max_len = 9500  # Adjust this as needed based on dataset characteristics\n",
    "\n",
    "# Trim or pad sequences\n",
    "def pad_or_trim_sequence(sequence, max_len):\n",
    "    if len(sequence) > max_len:\n",
    "        return sequence[:max_len]\n",
    "    else:\n",
    "        return sequence + 'N' * (max_len - len(sequence))  # Pad with 'N' for unknowns\n",
    "\n",
    "# Apply to all sequences\n",
    "processed_sequences = [pad_or_trim_sequence(seq, max_len) for seq in sequences]\n",
    "print(\"Sample processed sequence\", processed_sequences[0][:10],'...')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0732260c",
   "metadata": {},
   "source": [
    "\n",
    "### 2.2 One-Hot Encoding DNA Sequences\n",
    "\n",
    "To prepare the sequences for input into a 1D CNN, we will convert each base (A, C, G, T) into a one-hot encoding vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3e2483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define one-hot encoding for each base\n",
    "def one_hot_encode(sequence, max_len):\n",
    "    encoding_dict = {\n",
    "        'A': [1, 0, 0, 0],\n",
    "        'C': [0, 1, 0, 0],\n",
    "        'G': [0, 0, 1, 0],\n",
    "        'T': [0, 0, 0, 1],\n",
    "        'N': [0, 0, 0, 0]  # Placeholder for padding/unknowns\n",
    "    }\n",
    "    encoded_seq = [encoding_dict[base] for base in sequence]\n",
    "    return np.array(encoded_seq).T  # Transpose to shape (4, max_len)\n",
    "\n",
    "# Encode all sequences\n",
    "encoded_sequences = [one_hot_encode(seq, max_len) for seq in processed_sequences]\n",
    "encoded_sequences = np.stack(encoded_sequences)  # Stack into a single numpy array\n",
    "\n",
    "# Convert to PyTorch tensor and add a dimension for CNN input\n",
    "input_tensor = torch.tensor(encoded_sequences, dtype=torch.float32)  # Shape: (batch_size, 1, 4, max_len)\n",
    "print(\"Shape of input tensor:\", input_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ef3a92",
   "metadata": {},
   "source": [
    "\n",
    "### 2.3* Defining and Training the CNN\n",
    "\n",
    "Define the CNN architecture. Here I suggest using five convolutional layers and four fully connected layers as provided in the comments. Each convolutional layer is followed by a ReLU activation, max pooling, batch normalization, and dropout for regularization. But you are free to use the CNN that works for you.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50b2045",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the CNN model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, n_input, n_output, w, stride=3):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        # 5 convolutional layers\n",
    "\n",
    "\n",
    "        # Fully connected layers\n",
    "\n",
    "\n",
    "\n",
    "        # Max Pool Layer (used after each convolution)\n",
    "\n",
    "        \n",
    "        # Dropout and Batch Normalization\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolutional layers with batch norm, ReLU, and max pooling\n",
    "\n",
    "        \n",
    "        # Flatten the output\n",
    "\n",
    "        # Fully connected layers\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Define model with input and output dimensions\n",
    "n_input = None  # number of channels (one-hot encoding) * max_len\n",
    "n_output = None  # Number of classes\n",
    "\n",
    "# Initialize the model\n",
    "model = CNN(n_input=n_input, n_output=n_output, w=None, stride=None)  #The selection of an appropriate kernel size is crucial.\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422bd0ad",
   "metadata": {},
   "source": [
    "#### 2.4* Supervised Training of the model\n",
    "You may run the training process for as many epochs as you wish, but I suggest training for 100 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edac158f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define criterion and optimizer\n",
    "criterion = None\n",
    "optimizer = None\n",
    "\n",
    "# Convert labels to integer mapping and set up DataLoader\n",
    "label_map = {label: idx for idx, label in enumerate(set(labels))}\n",
    "mapped_labels = [label_map[lbl] for lbl in labels]\n",
    "\n",
    "labels_tensor = torch.tensor(mapped_labels, dtype=torch.long)\n",
    "dataset = TensorDataset(input_tensor, labels_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets in dataloader:\n",
    "        # Forward pass\n",
    "        \n",
    "        loss = torch.tensor([0.0])\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(dataloader):.4f}\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70a15fe",
   "metadata": {},
   "source": [
    "### 3.4 Evaluation on the validatoin set\n",
    "\n",
    "Similary to what we did in the previous version, we need to load the data from the validation partition and run the trained model for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf28364",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "fasta_file = \"HIV_data/val_data.fasta\"  # Replace with your file path\n",
    "sequences = []\n",
    "ids = []\n",
    "\n",
    "for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "    sequences.append(str(record.seq))\n",
    "    label = record.description[0]  # Extract the tid\n",
    "    ids.append(label)\n",
    "\n",
    "processed_sequences = [pad_or_trim_sequence(seq, max_len) for seq in sequences]\n",
    "\n",
    "\n",
    "# Encode all sequences\n",
    "encoded_sequences = [one_hot_encode(seq, max_len) for seq in processed_sequences]\n",
    "encoded_sequences = np.stack(encoded_sequences)  # Stack into a single numpy array\n",
    "\n",
    "# Convert to PyTorch tensor and add a dimension for CNN input\n",
    "val_features = torch.tensor(encoded_sequences, dtype=torch.float32)  # Shape: (batch_size, 4, max_len)\n",
    "print(\"Shape of input tensor:\", input_tensor.shape)\n",
    "\n",
    "\n",
    "val_labels = pd.read_csv(\"HIV_data/val_taxonomy.csv\")[\"taxonomy\"].to_list()\n",
    "mapped_val_labels = [label_map[lbl] for lbl in val_labels]\n",
    "val_labels_tensor = torch.tensor(mapped_val_labels, dtype=torch.long)\n",
    "\n",
    "# Create DataLoader for test set\n",
    "test_dataset = TensorDataset(val_features, val_labels_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Evaluation\n",
    "model.eval()  # Set model to evaluation mode\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.numpy())\n",
    "        all_labels.extend(targets.numpy())\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=\"Blues\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e091b3e7",
   "metadata": {},
   "source": [
    "> ### Question:\n",
    "> Was it easier to learn from engineered features or from raw data? Which architecture would you prefer in this task? Why? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev_iDeLUCS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
