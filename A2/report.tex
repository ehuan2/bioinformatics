\documentclass[10pt]{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{subfig}
\usepackage{listings}
\usepackage{hyperref}

\setlength{\oddsidemargin}{27mm}
\setlength{\evensidemargin}{27mm}
\setlength{\hoffset}{-1in}

\setlength{\topmargin}{27mm}
\setlength{\voffset}{-1in}
\setlength{\headheight}{0pt}
\setlength{\headsep}{0pt}

\setlength{\textheight}{235mm}
\setlength{\textwidth}{155mm}

%\pagestyle{empty}
\pagestyle{plain}

\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\renewcommand{\labelitemi}{$\diamond$}

\begin{document}
\baselineskip 12pt

\begin{center}
\textbf{\Large CS 482: Computational Techniques in Biological Sequence Analysis Homework \#2}\\

\vspace{0.5cc}
{ \sc Eric Haoran Huang$^{1}$}\\

\vspace{0.2 cm}

{\small $^{1}$e48huang@uwaterloo.ca, 20880126, e48huang}
 \end{center}

\begin{abstract}
  \noindent This assignment was an exercise in phylogeny, alignment-free methods and genome assembly.
\end{abstract}
\section*{Setup}
I used Python 3.10.12 on the student server machines in this run with external dependencies described in `requirements.txt'. Run `pip install requirements.txt' for proper setup.

\section*{Part 1: K-mer Composition}
All code and examples are found under `q1/'. This problem was tackled in a $O(nk)$ runtime where we had the following strategy:

\begin{enumerate}
  \item Grab the sequence using the \href{https://biopython.org/wiki/SeqIO}{BioPython}.
  \item Calculate the k-mer frequency array by iterating over all k-substrings (running $n - k + 1$ times with the length of the sequence being $n$).
  \begin{itemize}
    \item For each k-mer, we generate all possible k-mers, e.g.: `NA' will produce `AA', `CA', `GA', `TA'.
    \item We do this by iterating through each k-mer string one nucleotide at a time and keeping all possible k-mer prefixes. We extend each prefix by the possible next nucleotide base according to \href{https://en.wikipedia.org/wiki/Nucleic_acid_notation}{IUPAC notation}. This process takes $O(nk)$ times because we have $O(n)$ possible k-mers which we spend $O(k)$ time reconstructing all possible k-mers.
    \item Taking the total k-mers possible, weigh each possible string equally across each k-mer. Add this weight to a k-mer frequency array whose index is defined as mapping of the possible kmer string to its lexicographic index in the $4^k$ frequency matrix. This can be easily found by setting the weights of A to 0, C to 1, G to 2, T to 3 and then turning the string (alias of base 4) to base 10.
  \end{itemize}
  \item Return and print out to a file as needed.
\end{enumerate}

Run the code with the following line: `$\text{python kmer\_comp.py -i }<\text{input\_file}>\text{ -k }<\text{kmer-length}>$' with optional flags of `-o $<$output\_dir$>$' to output to the file under `$<$output\_dir$>$/$<$input\_file\_name$>$\_len\_$<$k-mer\_length$>$\_k-mers.txt' and `$--debug$' for more logging information.\\

I decided to use this method of averaging over the possible k-mers for ambiguous bases, i.e. if we have `NA' we give 0.25 frequency weights to `AA', `CA', `GA', `TA'. This assumes that given an ambiguous base, that there is equal chance of any base that represents it to be the true base. This might not necessarily be true, but is the best way to utilize the heuristic of the ambiguous base given.

\section*{Part 2: deBruijn Graph Construction}
All code and examples are found under `q2/'. This problem was tackled in a $O(nk \log n)$ runtime with the following strategy, where $n$ is the number of strings, $k$ is the length of the strings:
\begin{enumerate}
  \item Calculate the reverse complements of all the strings, taking $O(nk)$ time.
  \item Calculate the edge list, taking $O(nk)$ time of the joint set of the given set and the reverse complements.
  \item Return the sorted list, taking $O(k)$ time in comparison and needing $O(n \log n)$ comparisons, or done in $O(n k \log n)$ time.
\end{enumerate}

Run the code with the following line: `$\text{python build\_deBruijn.py -i }<\text{input\_file}>$' with optional flags of `-o $<$output\_dir$>$' to output to the file under `$<$output\_dir$>$/$<$input\_file\_name$>$\_deBruijn.txt' and `$--debug$' for more logging information.

\section*{Part 3: Alignment-free analysis of viral phylogenies}
All code and examples are found under `q3/'. This problem heavily utilized the existing libraries as follows:
\begin{enumerate}
  \item Downloaded the viral sequences using BioPython's \href{https://biopython.org/docs/1.76/api/Bio.Entrez.html#Bio.Entrez.efetch}{Entrez} package. This resulted in `.gb' files which were then cached and parsed through using BioPython again.
  \item Defined metrics as following:
  \begin{itemize}
    \item Euclidean distance. This metric is already a distance, no changes needed, used the L2-norm.
    \item Cosine similarity. This metric is not a distance and in fact, given that each component itself must be non-negative (given that frequency must be greater than or equal to 0), according to this \href{https://en.wikipedia.org/wiki/Cosine_similarity}{Wikipedia article$^*$}, it is bounded by $[0, 1]$, where it being $0$ means that it is completely dissimilar, and being $1$ means that it is an exact match. So, we need to translate this to distance which is bounded by $[0, \infty)$. Therefore, we need to map the $1$ to a $0$ and the $0$ to an infinity. A natural solution then is given $d$ as our cosine similarity, to take $\frac{1}{d} - 1$ as this maps $0$ to infinity, and also maps $1$ to $0$. I came up with this by noting that $\lim_{d \rightarrow 0}\frac{1}{d} = \infty$ but we need to shift it by $1$ for $\frac{1}{d} - 1$ to be $0$ at $d = 1$.
    \item Pearson Correlation. Similarly, I noted that according to \href{https://en.wikipedia.org/wiki/Pearson_correlation_coefficient}{Wikipedia$^*$}, that the expression will always lie between $-1$ and $1$, with $-1$ being completely negatively correlated and $1$ being completely positively correlated. This might look like if given two vectors, the $AA$ frequency increasing from one sequence to another will cause the frequency of $CC$ to decrease. So, a negative correlation means that they are more dissimilar, whereas a positive correlation means that they are more similar. Therefore, we need to map $-1$ to $\infty$ and $1$ to $0$. Then we have, given the Pearson correlation of $\rho$:
    
    \begin{align*}
      \lim_{\rho \rightarrow -1}\frac{1}{\rho + 1} = \infty, \frac{1}{\rho + 1}|_{\rho = 1} = \frac{1}{2}
    \end{align*}
    which matches the proper value for $\rho = -1$, however is wrong for $\rho = 1$. Then, we can do:
    \begin{align*}
      \frac{2}{\rho + 1} - 1 &= \frac{2 - \rho - 1}{\rho + 1}\\
      &= \frac{1 - \rho}{1 + \rho}
    \end{align*}
    which is what we implemented.
  \end{itemize}
  \item Used each of these distances on the kmer frequency vector calculated using q1, which created a distance matrix.
  \item Took this distance matrix and used the \href{https://biopython.org/docs/dev/api/Bio.Phylo.TreeConstruction.html#Bio.Phylo.TreeConstruction.DistanceTreeConstructor}{DistanceTreeConstructor} module to build a UPGMA tree.
\end{enumerate}
Run the code with the following line: `$\text{python build\_newick\_tree.py -i }<\text{input\_file}>\text{ -c }<\text{input\_candidate\_file}>$' with optional flags of `-o $<$output\_dir$>$' to output to that directory and `$--debug$' for more logging information. There is also a $-k$ flag to attempt this with different k-mer lengths.\\

Then, using this result we can answer the given questions.

\subsection*{Subtype Clustering}
The subtypes are very well clustered in the initial tree as shown in the following figures representing the Newick tree, generated through IcyTree. We can see that for every single tree, we have that the subtypes are all grouped together before grouping with any other subgroup.

\begin{figure}[htbp]
  \centering
    \includegraphics[width=\textwidth]{figures/euclidean_original.jpeg}
    \caption{Euclidean Distance Newick Tree of the original subtypes}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{figures/cosine_original.jpeg}
    \caption{Cosine Similarity Newick Tree of the original subtypes}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{figures/pearson_original.jpeg}
    \caption{Pearson Correlation of Newick Tree of the original subtypes}
\end{figure}
\newpage

\subsection*{Test Sequence}
With the test sequence, we see a slight change in the tree, where we simply add sequence 9 in.

\begin{figure}[htbp]
  \centering
    \includegraphics[width=\textwidth]{figures/euclidean_candidate.jpeg}
    \caption{Euclidean Distance Newick Tree with the test sample}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{figures/cosine_candidate.jpeg}
    \caption{Cosine Similarity Newick Tree with the test sample}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{figures/pearson_candidate.jpeg}
    \caption{Pearson Correlation of Newick Tree with the test sample}
\end{figure}
\newpage
\subsection*{Predicted subtype}
In all cases, we have that our subtype sequence 9 belongs to subtype2, and is a sibling of GQ199874.1.

\subsection*{Appropriate Metric}
In my opinion, I believe that cosine similarity to be the best metric as it is the most interpretable and grounded in what we want.\\

For Euclidean distance, a problem arises if we have two sequence frequencies of $[0, 1]$ and $[0, 2]$. If the direction is the same for the vectors, as it is here, then we know that the relative frequencies of the kmers are the same and therefore should not be penalized. However, for Euclidean distances, this is penalized.\\

Now, we can note that this is not penalized in both cosine similarity and Pearson correlation. For cosine similarity this is because the two vectors are in the exact same direction. For Pearson correlation this is because the correlation between the variables matches. In fact, one can show that my metrics I've used have the same derivative and grows at the same rate as each other. i.e. for any change in Pearson correlation distance metric, we would expect similar changes in the cosine similarity distance metric. In fact, looking at the previous figures, there is very little discrepancy between the two models. Therefore this boils down to how interpretable the two metrics are.\\

I argue that the cosine similarity is a more interpretable metric. Cosine similarity simply measures the angle between two vectors. This therefore measures the relative changes in components of both vectors. For frequency, we can imagine that this means how much the relative frequency matches with each other. This relative frequency can then be used as a heuristic for how similar two species are.\\

However, for the Pearson correlation, we are assuming that the various frequencies act essentially as independent samples of a distribution that might be correlated with another distribution. It is not exactly clear what $\rho = 0$ and its difference from $\rho = -1$, and how much this negative correlation should be taken into account as dissimilarity.\\

Therefore for its interpretability and usage as a metric, I believe that the cosine similarity should be the best metric to use.


\end{document}
