\documentclass[10pt]{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{subfig}
\usepackage{listings}
\usepackage{hyperref}

\setlength{\oddsidemargin}{27mm}
\setlength{\evensidemargin}{27mm}
\setlength{\hoffset}{-1in}

\setlength{\topmargin}{27mm}
\setlength{\voffset}{-1in}
\setlength{\headheight}{0pt}
\setlength{\headsep}{0pt}

\setlength{\textheight}{235mm}
\setlength{\textwidth}{155mm}

%\pagestyle{empty}
\pagestyle{plain}

\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\renewcommand{\labelitemi}{$\diamond$}

\begin{document}
\baselineskip 12pt

\begin{center}
\textbf{\Large CS 482: Computational Techniques in Biological Sequence Analysis Homework \#2}\\

\vspace{0.5cc}
{ \sc Eric Haoran Huang$^{1}$}\\

\vspace{0.2 cm}

{\small $^{1}$e48huang@uwaterloo.ca, 20880126, e48huang}
 \end{center}

\begin{abstract}
  \noindent This assignment was an exercise in phylogeny, alignment-free methods and genome assembly.
\end{abstract}
\section*{Setup}
I used Python 3.10.12 on the student server machines in this run with external dependencies described in `requirements.txt'. Run `pip install requirements.txt' for proper setup.

\section*{Part 1: K-mer Composition}
All code and examples are found under `q1/'. This problem was tackled in a $O(nk)$ runtime where we had the following strategy:

\begin{enumerate}
  \item Grab the sequence using the \href{https://biopython.org/wiki/SeqIO}{BioPython}.
  \item Calculate the k-mer frequency array by iterating over all k-substrings (running $n - k + 1$ times with the length of the sequence being $n$).
  \begin{itemize}
    \item For each k-mer, we generate all possible k-mers, e.g.: `NA' will produce `AA', `CA', `GA', `TA'.
    \item We do this by iterating through each k-mer string one nucleotide at a time and keeping all possible k-mer prefixes. We extend each prefix by the possible next nucleotide base according to \href{https://en.wikipedia.org/wiki/Nucleic_acid_notation}{IUPAC notation}. This process takes $O(nk)$ times because we have $O(n)$ possible k-mers which we spend $O(k)$ time reconstructing all possible k-mers.
    \item Taking the total k-mers possible, weigh each possible string equally across each k-mer. Add this weight to a k-mer frequency array whose index is defined as mapping of the possible kmer string to its lexicographic index in the $4^k$ frequency matrix. This can be easily found by setting the weights of A to 0, C to 1, G to 2, T to 3 and then turning the string (alias of base 4) to base 10.
  \end{itemize}
  \item Return and print out to a file as needed.
\end{enumerate}

Run the code with the following line: `$\text{python kmer\_comp.py -i }<\text{input\_file}>\text{ -k }<\text{kmer-length}>$' with optional flags of `-o $<$output\_dir$>$' to output to the file under `$<$output\_dir$>$/$<$input\_file\_name$>$\_len\_$<$k-mer\_length$>$\_k-mers.txt' and `$--debug$' for more logging information.\\

I decided to use this method of averaging over the possible k-mers for ambiguous bases, i.e. if we have `NA' we give 0.25 frequency weights to `AA', `CA', `GA', `TA'. This assumes that given an ambiguous base, that there is equal chance of any base that represents it to be the true base. This might not necessarily be true, but is the best way to utilize the heuristic of the ambiguous base given.

\section*{Part 2: deBruijn Graph Construction}
All code and examples are found under `q2/'. This problem was tackled in a $O(nk \log n)$ runtime with the following strategy, where $n$ is the number of strings, $k$ is the length of the strings:
\begin{enumerate}
  \item Calculate the reverse complements of all the strings, taking $O(nk)$ time.
  \item Calculate the edge list, taking $O(nk)$ time of the joint set of the given set and the reverse complements.
  \item Return the sorted list, taking $O(k)$ time in comparison and needing $O(n \log n)$ comparisons, or done in $O(n k \log n)$ time.
\end{enumerate}

Run the code with the following line: `$\text{python build\_deBruijn.py -i }<\text{input\_file}>$' with optional flags of `-o $<$output\_dir$>$' to output to the file under `$<$output\_dir$>$/$<$input\_file\_name$>$\_deBruijn.txt' and `$--debug$' for more logging information.

\section*{Part 3: Alignment-free analysis of viral phylogenies}
All code and examples are found under `q3/'. This problem heavily utilized the existing libraries as follows:
\begin{enumerate}
  \item Downloaded the viral sequences using BioPython's \href{https://biopython.org/docs/1.76/api/Bio.Entrez.html#Bio.Entrez.efetch}{Entrez} package. This resulted in `.gb' files which were then cached and parsed through using BioPython again.
  \item Defined metrics as following:
  \begin{itemize}
    \item Euclidean distance. This metric is already a distance, no changes needed, used the L2-norm.
    \item Cosine similarity. This metric is not a distance and in fact, given that each component itself must be non-negative (given that frequency must be greater than or equal to 0), according to this \href{https://en.wikipedia.org/wiki/Cosine_similarity}{Wikipedia article$^*$}, it is bounded by $[0, 1]$, where it being $0$ means that it is completely dissimilar, and being $1$ means that it is an exact match. So, we need to translate this to distance which is bounded by $[0, \infty)$. Therefore, we need to map the $1$ to a $0$ and the $0$ to an infinity. A natural solution then is given $d$ as our cosine similarity, to take $\frac{1}{d} - 1$ as this maps $0$ to infinity, and also maps $1$ to $0$. I came up with this by noting that $\lim_{d \rightarrow 0}\frac{1}{d} = \infty$ but we need to shift it by $1$ for $\frac{1}{d} - 1$ to be $0$ at $d = 1$.
    \item Pearson Correlation. Similarly, I noted that according to \href{https://en.wikipedia.org/wiki/Pearson_correlation_coefficient}{Wikipedia$^*$}, that the expression will always lie between $-1$ and $1$, with $-1$ being completely negatively correlated and $1$ being completely positively correlated. This might look like if given two vectors, the $AA$ frequency increasing from one sequence to another will cause the frequency of $CC$ to decrease. So, a negative correlation means that they are more dissimilar, whereas a positive correlation means that they are more similar. Therefore, we need to map $-1$ to $\infty$ and $1$ to $0$. Then we have, given the Pearson correlation of $\rho$:
    
    \begin{align*}
      \lim_{\rho \rightarrow -1}\frac{1}{\rho + 1} = \infty, \frac{1}{\rho + 1}|_{\rho = 1} = \frac{1}{2}
    \end{align*}
    which matches the proper value for $\rho = -1$, however is wrong for $\rho = 1$. Then, we can do:
    \begin{align*}
      \frac{2}{\rho + 1} - 1 &= \frac{2 - \rho - 1}{\rho + 1}\\
      &= \frac{1 - \rho}{1 + \rho}
    \end{align*}
    which is what we implemented.
  \end{itemize}
  \item Used each of these distances on the kmer frequency vector calculated using q1, which created a distance matrix.
  \item Took this distance matrix and used the \href{https://biopython.org/docs/dev/api/Bio.Phylo.TreeConstruction.html#Bio.Phylo.TreeConstruction.DistanceTreeConstructor}{DistanceTreeConstructor} module to build a UPGMA tree.
\end{enumerate}
Run the code with the following line: `$\text{python build\_newick\_tree.py -i }<\text{input\_file}>\text{ -c }<\text{input\_candidate\_file}>$' with optional flags of `-o $<$output\_dir$>$' to output to that directory and `$--debug$' for more logging information. There is also a $-k$ flag to attempt this with different k-mer lengths.\\

Then, using this result we can answer the given questions.

\subsection*{Subtype Clustering}
The subtypes are very well clustered in the initial tree as shown in the following figures representing the Newick tree, generated through IcyTree. We can see that for every single tree, we have that the subtypes are all grouped together before grouping with any other subgroup.

\begin{figure}[htbp]
  \centering
    \includegraphics[width=\textwidth]{figures/euclidean_original.jpeg}
    \caption{Euclidean Distance Newick Tree of the original subtypes}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{figures/cosine_original.jpeg}
    \caption{Cosine Similarity Newick Tree of the original subtypes}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{figures/pearson_original.jpeg}
    \caption{Pearson Correlation of Newick Tree of the original subtypes}
\end{figure}
\newpage

\subsection*{Test Sequence}
With the test sequence, we see a slight change in the tree, where we simply add sequence 9 in.

\begin{figure}[htbp]
  \centering
    \includegraphics[width=\textwidth]{figures/euclidean_candidate.jpeg}
    \caption{Euclidean Distance Newick Tree with the test sample}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{figures/cosine_candidate.jpeg}
    \caption{Cosine Similarity Newick Tree with the test sample}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{figures/pearson_candidate.jpeg}
    \caption{Pearson Correlation of Newick Tree with the test sample}
\end{figure}
\newpage
\subsection*{Predicted subtype}
In all cases, we have that our subtype sequence 9 belongs to subtype2, and is a sibling of GQ199874.1.

\subsection*{Appropriate Metric}
In my opinion, I believe that cosine similarity to be the best metric as it is the most interpretable and grounded in what we want.\\

For Euclidean distance, a problem arises if we have two sequence frequencies of $[0, 1]$ and $[0, 2]$. If the direction is the same for the vectors, as it is here, then we know that the relative frequencies of the kmers are the same and therefore should not be penalized. However, for Euclidean distances, this is penalized.\\

Now, we can note that this is not penalized in both cosine similarity and Pearson correlation. For cosine similarity this is because the two vectors are in the exact same direction. For Pearson correlation this is because the correlation between the variables matches. In fact, one can show that my metrics I've used have the same derivative and grows at the same rate as each other. i.e. for any change in Pearson correlation distance metric, we would expect similar changes in the cosine similarity distance metric. In fact, looking at the previous figures, there is very little discrepancy between the two models. Therefore this boils down to how interpretable the two metrics are.\\

I argue that the cosine similarity is a more interpretable metric. Cosine similarity simply measures the angle between two vectors. This therefore measures the relative changes in components of both vectors. For frequency, we can imagine that this means how much the relative frequency matches with each other. This relative frequency can then be used as a heuristic for how similar two species are.\\

However, for the Pearson correlation, we are assuming that the various frequencies act essentially as independent samples of a distribution that might be correlated with another distribution. It is not exactly clear what $\rho = 0$ and its difference from $\rho = -1$, and how much this negative correlation should be taken into account as dissimilarity.\\

Therefore for its interpretability and usage as a metric, I believe that the cosine similarity should be the best metric to use.

\section*{Part 4: Genome Assembly}
All code and examples are found under `q4/'. This problem was tackled with the following strategy:
\begin{enumerate}
  \item Grab the sequences and create an aggregate kmer-frequency array of length kmer by summing up the frequency over all possible kmers.
  \item With the optional flag `-t' or `--tol', allow the user to define what kmer frequency should be merged with its neighbours, defined as all possible kmers that are a single nucloetide away. This is the error correction step.
  \item Generate the deBrujin graph based on the total frequency of kmers by simply grabbing each kmer and setting the from kmer as the prefix and the to kmer as the suffix.
  \item Do some analysis on this graph, including finding all the components. From these components, remove all components except for the largest one. Empirically, most of these components should be safe to remove; with kmer of size 9, we see that the component sizes are $\{7679, 2, 2, 6, 2, 3, 3, 2\}$. Since these are small enough and would potentially cause deadends within the Eulerian path finding, we remove them.
  \item Next, we clean the graph optionally using the `--clean$\_$graph' flag which if used will add and remove potential edges that are making certain nodes' indegrees and outdegrees unbalanced. Given more time, I would have added the marking of a node's depth and the removal of any branches that lead nowhere.
  \item Finally, we find the Eulerian path (or a close one) by traversing over the edges randomly until we cannot anymore. Note this was done by turning the number of edges into weights for a probability distribution.
\end{enumerate}

Run the code with the following: `python assemble$\_$genome.py -i $<$input$\_$file$>$' with the following optional flags:
\begin{enumerate}
  \item `--output$\_$dir' specifies the output directory. If set, this also changes the output file to be of form: $<$input$>\_$assembled$\_$genome$\_$kmer$\_<$kmer-length$>\_$tol$\_<$tolerance$>\_$clean$\_$graph$\_<$clean-graph$>\_$seed$\_<$seed$>$.fna
  \item `--kmer$\_$len' specifies the kmer length to be used. Note that this makes the runtime exponentially slower due to the frequency array analysis. From experience, anything beyond $k=10$ might be unbearable, and not worth it; see the k-mer length ablations section.
  \item `--tol' specifies the upper bound for where we will move the kmer frequencies from one variable to another. See the error correction step ablations section for more details.
  \item `--clean$\_$graph' is a boolean flag which if set to true will add edges from some nodes to others and remove them where possible to balance the tree more. More details found under cleaning graph ablations.
\end{enumerate}

For my best run, use: 


\subsection*{BLAST Testing}
For BLAST testing, I simply tested against the \href{https://blast.ncbi.nlm.nih.gov/Blast.cgi?PROGRAM=blastn&PAGE_TYPE=BlastSearch&LINK_LOC=blasthome}{BLAST website}. This will be a verification alongside other heuristics for how good alignments are alongside how certain ablations led to better results.

\subsection*{Error Correction Step Ablations}
For this set of ablations, I looked at understanding how the tolerance flag will help or reduce the accuracy of the alignment. Note: To keep things consistent, I decided to use one seed of $42$ throughout my testing and ablations. While this might not be accurate of the entirety of the tests (and would require averaging over many seeds), it does show some possible situations of using certain seeds.\\

Below is a table of the resultant number of nodes, edges and components based on different tolerances for $k=9$:\\
\begin{table}[h]
\centering
\begin{tabular}{c|ccc}
Tolerance & 0 & 1 & 2 \\ \hline
Node Count & 8261 & 7699 & 7666 \\
\hline
Edge Count & 82770 & 82770 & 82770 \\
\hline
Component Count & 1 & 8 & 21
\end{tabular}
\caption{Graph Analysis on Various Tolerance Ablations}
\end{table}

Generally, there are less nodes (with the same number of edges as expected) but a higher number of components due to cutting off the bridges between components. This however, is not an issue because each component is composed of a very small number of kmers, meaning that they are likely to be errors than anything else. Switching these reads to an existing read is a much more likely situation.\\

This is backed up by the BLAST testing where

\subsection*{Graph Analysis; K-mer Length Ablations}

\subsection*{Cleaning Graph Ablations}


\end{document}
